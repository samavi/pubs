The popularity of  Graph Neural Networks (GNNs) in recent years underscores the need for further exploration of GNNs' trustworthiness. The confidence reported by GNN models in their predictions is an important aspect of trustworthiness, particularly in safety-critical domains such as healthcare. Recent proposals have identified that, unlike other deep learning models, GNNs exhibit under-confidence in their predictions. In this research, we propose Graph Confidence Error (GCE), a loss function to calibrate GNN model confidence during training. We compute the loss by quantifying the contribution of each data point to the model's confidence error and then use this value as a weight parameter in the loss function. 
We experimentally evaluated our approach for (1) three node classification tasks, including one heterogeneous and two homogeneous graphs, and (2) two graph classification tasks. The evaluation results demonstrate the reduction of the model's anticipated calibration error while preserving its overall performance.
